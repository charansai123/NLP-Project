{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group 5 \n",
    "#### B.Charan Sai               (AM.EN.U4CSE19314)\n",
    "#### Musunuru Varun         (AM.EN.U4CSE19336)\n",
    "#### V Venkata Praneeth    (AM.EN.U4CSE19358)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bef4048a374a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mPad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences as Pad\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!', 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!', 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.']\n",
      "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\"]\n"
     ]
    }
   ],
   "source": [
    "# load the data in 'movie_lines.txt' and the data in 'movie_conversations.txt'\n",
    "# we will use the lineID sets in movie_convs to reconstruct the real conversations in movie_lines\n",
    "\n",
    "movie_lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "movie_convs = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "print(movie_lines[:3])\n",
    "print(movie_convs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They do not!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id2sentence = {}\n",
    "for line in movie_lines:\n",
    "    line = line.split(' +++$+++ ')\n",
    "    if len(line) == 5:\n",
    "        id2sentence[line[0]] = line[4] \n",
    "        \n",
    "print(id2sentence['L1045'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203']]\n"
     ]
    }
   ],
   "source": [
    "# build a list containing all lineID sets\n",
    "conv_ID = []\n",
    "for lineIDs in movie_convs:\n",
    "    lineIDs= lineIDs.split(' +++$+++ ')[-1][1:-1].replace(' ','').replace(\"'\",\"\") # remove unnecessary symbols\n",
    "    conv_ID.append(lineIDs.split(','))\n",
    "print(conv_ID[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n"
     ]
    }
   ],
   "source": [
    "# classify questions and answers\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for lineIDs in conv_ID:\n",
    "    for i in range(len(lineIDs) - 1):\n",
    "        questions.append(id2sentence[lineIDs[i]])\n",
    "        answers.append(id2sentence[lineIDs[i + 1]])\n",
    "\n",
    "print(questions[0])\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # remove unnecessary characters in sentences\n",
    "    \n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"there's\", \"there is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again\n",
      "bos well i thought we would start with pronunciation if that is okay with you eos\n"
     ]
    }
   ],
   "source": [
    "clean_questions = []\n",
    "for question in questions: \n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    answer = 'bos ' + clean_text(answer) +' eos' \n",
    "    clean_answers.append(answer)\n",
    "\n",
    "print(clean_questions[0])\n",
    "print(clean_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions: 221616\n",
      "number of answers: 221616\n"
     ]
    }
   ],
   "source": [
    "print('number of questions: ' + str(len(clean_questions)))\n",
    "print('number of answers: ' + str(len(clean_answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74762\n",
      "74762\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "minlen = 2\n",
    "maxlen = 20\n",
    "filtered_questions_temp = []\n",
    "filtered_answers_temp = []\n",
    "\n",
    "\n",
    "for i,question in enumerate(clean_questions):\n",
    "    if len(question.split()) <= maxlen and len(question.split()) >= minlen:\n",
    "        filtered_questions_temp.append(question)\n",
    "        filtered_answers_temp.append(clean_answers[i])\n",
    "        \n",
    "filtered_questions = []\n",
    "filtered_answers = []\n",
    "\n",
    "\n",
    "for i, answer in enumerate(filtered_answers_temp):\n",
    "    if len(answer.split()) <= maxlen and len(answer.split()) >= minlen:\n",
    "        filtered_answers.append(answer)\n",
    "        filtered_questions.append(filtered_questions_temp[i])\n",
    "        \n",
    "print(len(filtered_answers))\n",
    "print(len(filtered_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos let me see what i can do eos\n",
      "gosh if only we could find kat a boyfriend\n"
     ]
    }
   ],
   "source": [
    "print(filtered_answers[0])\n",
    "print(filtered_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the input index sequence & vocabulary; build the output index sequence & vocabulary\n",
    "\n",
    "# we build 2 different vocabulary because it is convinient, some other code only use one\n",
    "# this method is also used in machine translation model because we need 2 different vocabulary in different language\n",
    "\n",
    "# define the tokenizer\n",
    "vocabsize = 8000\n",
    "\n",
    "# vocabulary size is 8000, and we use UNK to replace those words are not frequently used\n",
    "question_tokenizer = Tokenizer(num_words = vocabsize+1, oov_token = 'unk')\n",
    "answer_tokenizer = Tokenizer(num_words = vocabsize+1,oov_token = 'unk')\n",
    "\n",
    "# tokenize the questions and answers\n",
    "question_tokenizer.fit_on_texts(filtered_questions)\n",
    "answer_tokenizer.fit_on_texts(filtered_answers)\n",
    "\n",
    "# build the input sequence and output sequence\n",
    "q_sequences = question_tokenizer.texts_to_sequences(filtered_questions)\n",
    "a_sequences = answer_tokenizer.texts_to_sequences(filtered_answers)\n",
    "\n",
    "# pad sequences in same length so that we can train them in model\n",
    "q_pad = Pad(q_sequences, padding = 'post')\n",
    "a_pad = Pad(a_sequences, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data\n",
    "q_token_json = question_tokenizer.to_json()\n",
    "a_token_json = answer_tokenizer.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/questions.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(q_token_json, ensure_ascii=False))\n",
    "    f.close()\n",
    "\n",
    "with open('preprocessed_data/answers.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(a_token_json, ensure_ascii=False))\n",
    "    f.close()\n",
    "\n",
    "np.savez('preprocessed_data/data.npz', q_pad, a_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
